{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML STEPS\n",
    "\n",
    "\n",
    "1 - Data Collection\n",
    "\n",
    "\n",
    "2 - Data Preparation\n",
    "    Duplicates, correct errors, deal with missing values, normalization, data type conversions, etc.)\n",
    "    Randomize data\n",
    "    Check for class imbalances\n",
    "    Split into training and evaluation sets \n",
    "    \n",
    "    \n",
    "3 - Choose a Model  \n",
    "\n",
    "\n",
    "4 - Train the Model\n",
    "\n",
    "\n",
    "5 - Evaluate the Model - 80/20, 70/30, or similar \n",
    "\n",
    "\n",
    "6 - Parameter Tuning - 1) Param grid 2) K-fold 3) Grid search\n",
    "(number of training steps, learning rate, initialization values and distribution, etc.) \n",
    "\n",
    "\n",
    "7 - Make Predictions    \n",
    "Using further (test set) data which have, until this point, been withheld from the model (and for which class labels are known), are used to test the model; a better approximation of how the model will perform in the real world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: #to show plots\n"
     ]
    }
   ],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  #to show plots\n",
    "%config Completer.use_jedi = False #to autocomplete with tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cursory view of data\n",
    "with open ('data/adult.names') as f:\n",
    "    notes = f.read()\n",
    "print(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dta = pd.read_csv('data/adult.data.cleaned.csv.gz', compression = 'gzip')\n",
    "test = pd.read_csv('data/adult.test.cleaned.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = test\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#EXPLORING THE DATA\n",
    "\n",
    "# import pandas_profiling as pp ; pp.ProfileReport(df) #GREAT TOOL FOR QUICK DATA OVERVIEW\n",
    "\n",
    "#Check dataset balance\n",
    "# sns.countplot(x = 'target', data = df, palette = 'RdBu_r')\n",
    "\n",
    "#df.sample(5)\n",
    "#df.head()\n",
    "#df.tail()\n",
    "#df.info()\n",
    "#df.describe(include = 'all')\n",
    "#df.min() #helps detect negative values!\n",
    "#df.shape\n",
    "#df.column.unique()\n",
    "#pd.set_option('display.max_columns', 100)\n",
    "#df['Name'][df['Opportunity value'] > 70].describe()\n",
    "#df.col.value_counts()#(normalize=True).plot()\n",
    "##pd.plotting.scatter_matrix(df.head(20), figsize = (15,5), diagonal = 'hist' )\n",
    "#plt.xticks(rotation = 90)\n",
    "#df.col.value_counts().count\n",
    "#df.sort_values(by='column')\n",
    "#df[df['Events'].isin(['Rain','Snow'])]\n",
    "\n",
    "#Display multiple objects\n",
    "# display(df1, df2)\n",
    "\n",
    "###VISUALIZING HISTOGRAM FOR EACH COLUMN WITH NUM VALUES #warning: time-consuming## \n",
    "# df.hist(bins = 15, figsize = (20,15))\n",
    "# plt.show()\n",
    "\n",
    "#plt.bar(df.Country.head(), df.Rank.head())\n",
    "\n",
    "#VISUALIZING BOTH CATEGORICAL AND NUMERICAL DATA:\n",
    "# #explore the data graphically\n",
    "# fig = plt.figure(figsize = (20,20))\n",
    "# cols = 3\n",
    "# rows = math.ceil(float(df.shape[1] / cols))\n",
    "\n",
    "# for i, column in enumerate(list(df.columns)):\n",
    "#     ax = fig.add_subplot(rows, cols, i + 1)\n",
    "#     ax.set_title(column)\n",
    "#     if df.dtypes[column] == np.object:\n",
    "#         df[column].value_counts().plot(kind = 'bar', axes = ax)\n",
    "#     else:\n",
    "#         df[column].hist(axes = ax)\n",
    "#         plt.xticks(rotation = 'vertical')\n",
    "# plt.subplots_adjust(hspace = 0.7, wspace = 0.2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#INSPECTING FOR WRONG DATA, DUPLICATES OR OUTLIERS\n",
    "\n",
    "#df.isnull().any()\n",
    "#df.isnull().values.any()\n",
    "#df.isna().sum() #COUNT MISSING VALUES PER COLUMN\n",
    "#df[~df.isin([np.nan, np.inf, -np.inf]).any(1)] #FINDING NON NUMERIC OR TEXT VALUES\n",
    "#recipes.description.str.contains('[Bb]reakfast').sum()\n",
    "#CREATE GRAPH FOR NULL VALUES: sns.heatmap(df.isnull(),yticklabels = False, cbar = False, cmap = 'viridis')\n",
    "#df[(df == '?').any(axis=1)].count() #any row with question mark\n",
    "\n",
    "#total_null = df.isna().sum().sort_values(ascending = False)\n",
    "#percent = (df.isna().sum() / df.isna().count()).sort_values(ascending = False)\n",
    "#missing_data = pd.concat([total_null, percent], axis = 1, keys = ['Total', 'Percent'])\n",
    "\n",
    "#df.col.duplicated().value_counts()\n",
    "#df.duplicated().sum()\n",
    "#df.drop_duplicates(keep = 'first', inplace = True)\n",
    "#df.loc[:, ['col1', 'col2']]\n",
    "\n",
    "#Generate new column with missing data from another column\n",
    "# df['NaN'] = df['col'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CLEANING AND REARRANGING THE DATA\n",
    "\n",
    "#df.dropna()\n",
    "#df.col.fillna(0)\n",
    "#df.col.fillna(df['col'].mean())\n",
    "#df.rename(columns = {'col1':'col1name'})\n",
    "#df.columns.str.upper()\n",
    "\n",
    "#df.columns = [col [1] for col in df.columns] CONVERT multi INDEX into simple 1 level index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PLOTTING THE DATA TO VISUALIZE ANOMALIES\n",
    "\n",
    "#df.plot(figsize = (15,5))\n",
    "#df.marital_status.value_counts().plot(kind = 'bar')\n",
    "#plt.hist(df.age, bins = 20)\n",
    "#plt.scatter(df.occupation, df.age)\n",
    "#plt.plot(df.capital_gain)\n",
    "\n",
    "#SHOWING TITLES\n",
    "# plt.title = ('Distribution by age')\n",
    "# plt.xlabel = ('Player name')\n",
    "# plt.ylabel = ('Age')\n",
    "\n",
    "#PLOTTING BY GROUP\n",
    "# test = df.groupby('auction_type')\n",
    "# test.get_group('5 day auction')[['price','openbid']].plot(figsize = (15,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#VISUALIZE CORRELATIONS\n",
    "#df.corr(method = 'spearman') #other methods are 'kendall' and 'pearson'\n",
    "#df['age'].corr(df['capital_gain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1fb00685026e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m####MULTI CORR SEPARATED####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#Distplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "##VISUALIZE CORRELATIONS GRAPHICALLY\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "####SIMPLE REGRESSION LINE PLOT####\n",
    "#sns.pairplot(df, x_vars = ['capital_loss', 'hours_per_week', 'education_num'], y_vars = 'age', size = 7, aspect = 0.7, kind = 'reg')\n",
    "\n",
    "####MULTI CORR SEPARATED####\n",
    "sns.pairplot(data = df)\n",
    "\n",
    "#Distplot\n",
    "#sns.distplot(df['column'])\n",
    "\n",
    "#SIMPLE CORR\n",
    "sns.heatmap(df.corr())\n",
    "\n",
    "#GRAPH 1 CORR\n",
    "# # Compute the correlation matrix\n",
    "# corr = df.corr()\n",
    "# # Set up the matplotlib figure\n",
    "# f, ax = plt.subplots(figsize=(15, 10))\n",
    "# # Draw the heatmap with the mask and correct aspect ratio\n",
    "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "#             square=True, linewidths=.5)\n",
    "\n",
    "####GRAPH 2 CORR HEATMAP####\n",
    "# corr = df.corr()\n",
    "# corr = (corr)\n",
    "# sns.heatmap(corr,\n",
    "#            xticklabels = corr.age.values,\n",
    "#            yticklabels = corr.capital_gain.values)\n",
    "\n",
    "####GRAPH 3 BOXPLOT####\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# fig = sns.boxplot(x = df.marital_status, y = df.age)\n",
    "\n",
    "#GRAPH 4 SIMPLE HORIZONTAL\n",
    "# f, ax = plt.subplots(figsize=(15, 5))\n",
    "# sns.countplot(y='col1', hue='col2', data=df).set_title('Title')\n",
    "\n",
    "#GRAPH 5 SEPARATED BOXPLOT\n",
    "# df.plot(kind = 'box', subplots = True, layout = (2,2), sharex = False, sharey = False)\n",
    "# plt.show()\n",
    "\n",
    "####GRAPH 6 SEPARATED HIST####\n",
    "# df.hist(figsize=(20, 20))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MANIPULATING THE DATA TO VIEW IT FROM DIFFERENT ANGLES\n",
    "\n",
    "#df.sort_values(['col1','col2'], ascending = False).head(20)\n",
    "#df.groupby(df.col).count().plot(kind = 'bar')\n",
    "#df.groupby('col1')['col2'].mean().sort_values(ascending = False).head(10)\n",
    "# test = df.groupby('auction_type') # print(test.get_group('3 day auction').describe())\n",
    "#pd.crosstab(df.col1, df.col2).apply(lambda x: x/x.sum(), axis=1)\n",
    "#df['young_male'] = ((df.gender == 'M') & (df.age < 30)).map({True: 'young male', False: 'other'})\n",
    "#df.pivot_table('Exited', index = ['Geography', 'Gender'], columns = 'Tenure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#REPLACING VALUES\n",
    "#Replace symbol in whole column\n",
    "#df['Seasons'] = df['Seasons'].str.replace('–', '**', regex = True)\n",
    "\n",
    "#DROP multiple columns\n",
    "#df.drop(['sex_enc' , 'enc_country'], axis = 1, inplace = True)\n",
    "\n",
    "#Find text with regex and replace with nothing\n",
    "# df = df.replace({\n",
    "#     'Value':'[A-Za-z]', \n",
    "#     'Wage': '[A-Za-z]',\n",
    "# },'',regex = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#APPLYING FORMULAE\n",
    "\n",
    "# def euro(cell):\n",
    "#     cell = cell.strip('€')\n",
    "#     return cell\n",
    "# df.Wage = df.Wage.apply(euro)\n",
    "\n",
    "\n",
    "#Insert value in cell depending on values from other cells\n",
    "# def impute_age(cols):\n",
    "#     age = cols[0]\n",
    "#     Pclass = cols[1]\n",
    "#     if pd.isnull(Age):\n",
    "#         if Pclass == 1:\n",
    "#             return 37\n",
    "#         elif Pclass == 2:\n",
    "#             return 29\n",
    "#         else:\n",
    "#             return 24\n",
    "#     else:\n",
    "#         return Age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CHANGING DATA TYPES\n",
    "#changing values to float\n",
    "#df[['Value','Wage','Age']].apply(pd.to_numeric, errors = 'coerce')\n",
    "#df.column.astype(float)\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CHECK FOR NEGATIVE VALUES BY COLORING THEM\n",
    "def color_negative_red(val):\n",
    "    color = 'red' if val < 38 else 'black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "test = df[['age','capital_gain']]\n",
    "colored = test.head().style.applymap(color_negative_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CHANGING CATEGORICAL TO CONTINUOUS DATA\n",
    "def gender_to_numeric(x):\n",
    "    if x == 'Male':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applying a Python list comprehension to the Pandas' data\n",
    "y = df.groupby('occupation')\n",
    "x = [el for el in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Classification?) Check dataset balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#If the data is imbalanced we need to balance it by creating new records \n",
    "#or reducing the number of records of the dependent variable\n",
    "\n",
    "#Checking imbalance. In this case target var is called \"Exited\"\n",
    "df.groupby('Exited').size()\n",
    "\n",
    "#Creating the under sampling data \n",
    "print(X.shape, y.shape)\n",
    "\n",
    "####WAY TO AUTOMATICALY UPSAMPLE OR DOWNSAMPLE#####\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "os = RandomOverSampler(ratio = 1) #ratio will sample so that the ratio of one feature and another is 50% and 50% respectively\n",
    "X_train_res, y_train_res = os.fit_sample(X, y)\n",
    "\n",
    "###################################################\n",
    "\n",
    "# #WAY TO DOWNSAMPLE (REDUCE NUMBER OF RECORDS)\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "# nm = NearMiss(random_state = 42)\n",
    "# X_res, y_res = nm.fit_sample(X, y)\n",
    "\n",
    "# #WAY TO UPSAMPLE (INCREASE NUMBER OF RECORDS)\n",
    "# from imblearn.combine import SMOTETomek\n",
    "# smk = SMOTETomek(random_state = 42)\n",
    "# X_res, y_res = smk.fit_sample(X, y)\n",
    "\n",
    "print(X_res.shape, y_res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#One of the fastest ways\n",
    "df.columns[df.dtypes == 'object']\n",
    "pd.get_dummies(df, df.columns[df.dtypes == 'object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/\n",
    "\n",
    "#CAN BE DONE FASTER WITH PANDAS DUMMIES)))#######\n",
    "df = pd.get_dummies(df, columns = ['col1', 'col2'], drop_first = True)\n",
    "#################################################\n",
    "\n",
    "#Label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['new_col'] = le.fit_transform(df['Geography'])#df['Geography'].astype(str) may be necessary\n",
    "\n",
    "# Loop to encode string input values as integers for multiple columns\n",
    "features = []\n",
    "for i in range(0, X.shape[1]):\n",
    "    le = LabelEncoder()\n",
    "    feature = le.fit_transform(X[:,i])\n",
    "    features.append(feature)\n",
    "encoded_x = numpy.array(features)\n",
    "encoded_x = encoded_x.reshape(X.shape[0], X.shape[1])\n",
    "\n",
    "#One hot encoding for categorical information\n",
    "#We can one hot encode each feature after we have label encoded it.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "df['new_col'] = enc.fit_transform(df['Geography'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using SickitLearn to transform mixed type columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL DATA CLEANSING AND CLASSIFICATION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data from Titanic dataset.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "\n",
    "#We create the preprocessing pipelines for both numeric and categorical data\n",
    "numeric_features = ['age','fare']\n",
    "\n",
    "numeric_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "#Append classifier to preprocessing pipeline\n",
    "#Now we have a full prediction pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "X = df.drop('survived', axis = 1)\n",
    "y = df['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format to load data into X and y for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#EXAMPLES ON HOW TO LOAD DATA IN X AND Y for the predictive model\n",
    "\n",
    "#Example 1\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "\n",
    "#Example 2\n",
    "X = np.array(df.drop('Bankruptcy' , 1))\n",
    "y = np.array(df['Bankruptcy'])\n",
    "\n",
    "#Example 3\n",
    "X = df.iloc[:,[2,3]].values\n",
    "y = df.iloc[:,4].values\n",
    "\n",
    "#Example 4\n",
    "array = df.values\n",
    "X = array[:, 0:4]\n",
    "y = array[:, 4]\n",
    "\n",
    "#Example 5\n",
    "X = df[c for c in columns if c not in ['target']]\n",
    "y = df['target']\n",
    "\n",
    "#Turning dataframe into array of values\n",
    "dataset = df.values\n",
    "\n",
    "#Saving / save the model\n",
    "import joblib\n",
    "joblib.dump(model, 'filename.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle data before train_test_split and fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(mnist.data[mask], mnist.target[mask], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Sickit Learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To test all scikit models in one go, explore:\n",
    "#https://github.com/ypeleg/HungaBunga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#BASIC SCICKIT LEARN LINEAR REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as LinReg\n",
    "X = df.index.values.reshape(-1, 1)\n",
    "y = df.education_num.values\n",
    "\n",
    "reg = LinReg()\n",
    "reg.fit(X,y)\n",
    "y_preds = reg.predict(X)\n",
    "print(reg.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(x = X, y = y_preds, c = 'b') #\"c\" means color. b means \"blue\"\n",
    "plt.scatter(x = X, y = y, c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style = \"ticks\")\n",
    "# creates FacetGrid\n",
    "g = sns.FacetGrid(df, col = \"education_num\", height = 25)\n",
    "g.map(plt.hist, \"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format to get predictions from new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We create a df out of the array of data\n",
    "new_df = pd.DataFrame([[6, 168, 72, 35, 0, 43.6, 0.627, 65]])\n",
    "\n",
    "# We scale those values like the others\n",
    "new_df_scaled = scaler.transform(new_df)\n",
    "\n",
    "# We predict the outcome\n",
    "prediction = svc.predict(new_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DIMENSIONALITY REDUCTION\n",
    "\n",
    "https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7ehttps://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e\n",
    "\n",
    "#Feature selection is reducing number of features among existing features. \n",
    "\n",
    "\n",
    "Univariate feature selection examines each feature individually to determine the strength of the relationship of the feature with the response variable.\n",
    "\n",
    "\n",
    "Feature engineering is manually generating new features from existing features\n",
    "#Curse of diminesionality - more dimensions need more processing power, time, increases overfitting, limits number of algos to be used\n",
    "\n",
    "To reduce dimensions:\n",
    "\n",
    "\n",
    "#HEATMAPS to see corr\n",
    "We will only select features which have a correlation of above 0.5 (taking absolute value) with the output variable.\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "LINEAR DIM REDUCTION\n",
    "\n",
    "1) PCA - used for dimensionality reduction in continuous data, // PCA rotates and projects data along the direction of increasing variance. The features with the maximum variance are the principal components.\n",
    "\n",
    "2) Factor analysis - values are expressed as functions and Gaussian noise is added.\n",
    "\n",
    "3) LDA - projects data in a way in which the class separability is maximised\n",
    "<br><br><br>\n",
    "\n",
    "NON-LINEAR DIM REDUCTION (aka MANIFOLD LEARNING METHODS)\n",
    "\n",
    "Manifold hypothesis: in a high dimensional structure, most relevant information is concentrated in small number of low dimensional manifolds. \n",
    "\n",
    "1) Multi-Dimensional Scaling (MDS) Projects data to a lower dimension such that data points that are close to each other (in terms of Euclidean distance) in the higher dimension are close in the lower dimension as well.\n",
    "\n",
    "\n",
    "2) Isometric Feature Mapping (Isomap) : Projects data to a lower dimension while preserving the geodesic distance (rather than Euclidean distance as in MDS). Geodesic distance is the shortest distance between two points on a curve.\n",
    "\n",
    "\n",
    "3) Locally Linear Embedding (LLE): Recovers global non-linear structure from linear fits. Each local patch of the manifold can be written as a linear, weighted sum of its neighbours given enough data.\n",
    "\n",
    "4) Hessian Eigenmapping (HLLE): Projects data to a lower dimension while preserving the local neighbourhood like LLE but uses the Hessian operator to better achieve this result and hence the name.\n",
    "    \n",
    "5) Spectral Embedding (Laplacian Eigenmaps): mapping nearby inputs to nearby outputs. It preserves locality rather than local linearity\n",
    "\n",
    "\n",
    "6) t-distributed Stochastic Neighbor Embedding (t-SNE): Computes the probability that pairs of data points in the high-dimensional space are related and then chooses a low-dimensional embedding which produces a similar distribution.\n",
    "\n",
    "#F-Test checks for and only captures linear relationships between features and labels. A highly correlated feature is given higher score and less correlated features are given lower score. Correlation is highly deceptive as it doesn’t capture strong non-linear relationships. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.\n",
    "\n",
    "\n",
    "\n",
    "Auto-encoders\n",
    "\n",
    "Another popular dimensionality reduction method that gives spectacular results are auto-encoders, a type of artificial neural network that aims to copy their inputs to their outputs. They compress the input into a latent-space representation, and then reconstructs the output from this representation. An autoencoder is composed of two parts :\n",
    "\n",
    "    Encoder: compresses the input into a latent-space representation.\n",
    "    Decoder: reconstructs the input from the latent space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#EXAMPLE OF DIM REDUCTION WITH PCA\n",
    "#We first scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "scaled_data = scaler.transform(df)\n",
    "#scaled_data.shape\n",
    "\n",
    "#Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2) #2 is the number of dimension we want\n",
    "pca.fit(scaled_data)\n",
    "x_pca = pca.transform(scaled_data)\n",
    "#scaled_data.shape\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(x_pca[:,0], x_pca[:,1],c=df['target_col'])\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FEATURE SELECTION - FILTER METHOD\n",
    "##### WARNING: FILTER METHOD TO BE USED MAINLY ON SMALL DATASETS\n",
    "\n",
    "## FILTER METHOD is less accurate. It is great while doing EDA, it can also be used for checking multi co-linearity in data.\n",
    "\n",
    "https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "\n",
    "#Loading the dataset\n",
    "x = load_boston()\n",
    "df = pd.DataFrame(x.data, columns = x.feature_names)\n",
    "df[\"MEDV\"] = x.target\n",
    "X = df.drop(\"MEDV\",1)   #Feature Matrix\n",
    "y = df[\"MEDV\"]          #Target Variable\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#FEATURE SELECTION BASED ON CORRELATION\n",
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"MEDV\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features\n",
    "\n",
    "#WE MUST FURTHER ELIMINATE FEATURES CORRELATED WITH EACH OTHER (FOR LINEAR REG)\n",
    "print(df[[\"LSTAT\",\"PTRATIO\"]].corr())\n",
    "print(df[[\"RM\",\"LSTAT\"]].corr())\n",
    "'''From the above code, we see that the variables RM and LSTAT are highly correlated with each other (-0.613808). \n",
    "Hence we would keep only one variable and drop the other. \n",
    "We will keep LSTAT since its correlation with MEDV is higher than that of RM.\n",
    "After dropping RM, we are left with two feature, LSTAT and PTRATIO. \n",
    "These are the final features given by Pearson correlation.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper and Embedded methods give more accurate results but as they are computationally expensive, these methods are suited for when you have fewer features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE SELECTION - WRAPPER METHOD - STATSMODELS(SM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-4b504b24edf3>, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-4b504b24edf3>\"\u001b[1;36m, line \u001b[1;32m41\u001b[0m\n\u001b[1;33m    print('P-values are: ', model.pvalues)\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#BACKWARD ELIMINATION\n",
    "'''we feed all the possible features to the model, \n",
    "then iteratively remove \n",
    "the worst performing features one by one\n",
    "Metric to evaluate feature performance is\n",
    "pvalue above 0.05 (to keep the feature).'''\n",
    "\n",
    "#FORWARD SELECTION\n",
    "\n",
    "'''we feed the features to a Machine Learning algorithm one by one.\n",
    "\n",
    "1) Features with high correlation are more linearly dependent \n",
    "and hence have almost the same effect on the dependent variable. \n",
    "So, when two features have high correlation, \n",
    "we can drop one of the two features.\n",
    "\n",
    "2) P-value or \"probability value\" or \"asymptotic significance\"\n",
    "is a probability value for a given statistical model that, \n",
    "if the null hypothesis is true, \n",
    "a set of statistical observations more commonly known \n",
    "as the \"statistical summary\" is greater than or equal in magnitude \n",
    "to the observed results.\n",
    "\n",
    "Removal of different features from the dataset will have different effects \n",
    "on the p-value for the dataset. \n",
    "We can remove different features and measure the p-value in each case. \n",
    "These measured p-values can be used to decide\n",
    "whether to keep a feature or not. \n",
    "If we see a high p value together with negative coef, \n",
    "we must check for colinearity.\n",
    "'''\n",
    "\n",
    "#Performing OLS regression to find out features\n",
    "#with p value lower than a certain treshold:\n",
    "#Note: adding constant column of ones is mandatory for sm.OLS model \n",
    "#(OLS= Ordinary Least Squares)\n",
    "import statsmodels.api as sm\n",
    "X1 = sm.add_constant(X)\n",
    "model = sm.OLS(y, X1).fit()\n",
    "print(model.summary()\n",
    "print('P-values are: ', model.pvalues)\n",
    "\n",
    "#Backward Elimination\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "selected_features_BE = cols\n",
    "print()\n",
    "print('Selected features after loop are: ', selected_features_BE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE SELECTION - WRAPPER METHOD - RFE (RECURSIVE FEATURE ELIMINATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Recursively removing attributes and building a model \n",
    "#on those attributes that remain\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, 7)\n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)  \n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Now we need to find the optimum number of features, \n",
    "for which the accuracy is the highest. \n",
    "We do that by using a loop starting with 1 feature and going up to 13. \n",
    "We then take the one for which the accuracy is highest.'''\n",
    "\n",
    "#no of features\n",
    "nof_list=np.arange(1,13)            \n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#As seen from above code, the optimum number of features is 10. \n",
    "'''We now feed 10 as number of features to RFE \n",
    "and get the final set of features given by RFE:'''\n",
    "\n",
    "cols = list(X.columns)\n",
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, 10)             \n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)  \n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)              \n",
    "temp = pd.Series(rfe.support_,index = cols)\n",
    "selected_features_rfe = temp[temp==True].index\n",
    "print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION - EMBEDDED METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''We iterate over the model training process and carefully \n",
    "extract those features which contribute the most \n",
    "to the training for a particular iteration, \n",
    "penalizing a feature given a threshold.\n",
    "Here we will do feature selection using Lasso regularization. \n",
    "If the feature is irrelevant, lasso penalizes its coefficient making it 0. \n",
    "Features with coefficient = 0 are removed.'''\n",
    "\n",
    "reg = LassoCV()\n",
    "reg.fit(X, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\n",
    "coef = pd.Series(reg.coef_, index = X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "imp_coef = coef.sort_values()\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\");plt.show()\n",
    "\n",
    "#Here Lasso model has taken all the features except NOX, CHAS and INDUS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of SelectKBest (chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#EXAMPLE OF USING SICKIT LEARN TO SELECT THE TWO BEST FEATURES\n",
    "#https://github.com/knathanieltucker/bit-of-data-science-and-scikit-learn/blob/master/notebooks/FeatureSelection.ipynb\n",
    "#https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "\n",
    "fit = bestfeatures.fit(X,np.ravel(y))\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of ExtRaTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-30e58cc4e68e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfeat_importances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#FEATURE SELECTION USING EXTRA TREES CLASSIFIER\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_)\n",
    "feat_importances = pd.Series(model.feature_importances_, index = X.columns)\n",
    "feat_importances.nlargest(10).plot(kind = 'barh')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of RFECV(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RECURSIVE FEATURE ELIMINATION with cross validation (using Random Forest)\n",
    "'''Assigns weights to features (e.g., the coefficients of a linear model). \n",
    "Features whose weights are the smallest are pruned out.'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "m = RFECV(RandomForestClassifier(), scoring = 'accuracy')\n",
    "m.fit(X,y)\n",
    "m.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of SelectFromModel (LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FEATURE SELECTION USING SelectFromModel AND SVC\n",
    "\n",
    "'''features are considered unimportant and removed, \n",
    "if the corresponding coef_ or featureimportances values \n",
    "are below the provided threshold parameter.\n",
    "Available heuristics are “mean”, “median” \n",
    "and float multiples of these like “0.1*mean”.'''\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "m = SelectFromModel(LinearSVC(C=0.01, penalty='l1', dual=False))\n",
    "m.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of SelectFromModel (LassoCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FEATURE SELECTION USING SelectFromModel AND LASSO\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "m = SelectFromModel(LassoCV(cv = 5))\n",
    "m.fit(X,y)\n",
    "#m.transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique to prun decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRUNNING DECISION TREES\n",
    "#https://stackoverflow.com/questions/49428469/pruning-decision-trees\n",
    "'''Directly restricting the lowest value (number of occurences of a particular class) of a leaf\n",
    "cannot be done with min_impurity_decrease or any other built-in stopping criteria.\n",
    "I think the only way you can accomplish this without changing the source code \n",
    "of scikit-learn is to post-prune your tree. \n",
    "To accomplish this, you can just traverse the tree \n",
    "and remove all children of the nodes with minimum class count less than 5 \n",
    "(or any other condition you can think of). I will continue your example:'''\n",
    "\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "def prune_index(inner_tree, index, threshold):\n",
    "    if inner_tree.value[index].min() < threshold:\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "    # if there are children, visit them as well\n",
    "    if inner_tree.children_left[index] != TREE_LEAF:\n",
    "        prune_index(inner_tree, inner_tree.children_left[index], threshold)\n",
    "        prune_index(inner_tree, inner_tree.children_right[index], threshold)\n",
    "\n",
    "print(sum(dt.tree_.children_left < 0))\n",
    "# start pruning from the root\n",
    "prune_index(dt.tree_, 0, 5)\n",
    "sum(dt.tree_.children_left < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d018a874c3ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "##https://www.andreagrandi.it/2018/04/14/machine-learning-pima-indians-diabetes/\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler as Scaler\n",
    "\n",
    "scaler = Scaler()\n",
    "scaler.fit(train_set)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Example 1\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "#Example 2, multiple cols\n",
    "standardScaler = StandardScaler()\n",
    "columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Applying multipe algorithms to classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A typical ML implementation for a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c67eb9e658ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set_scaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_set_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "#https://www.andreagrandi.it/2018/04/14/machine-learning-pima-indians-diabetes/\n",
    "# Import all the algorithms we want to test\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Prepare an array with all the algorithms\n",
    "models = []\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('LSVC', LinearSVC()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('RFC', RandomForestClassifier()))\n",
    "models.append(('SVM', SVC(gamma = 'auto')))\n",
    "\n",
    "#REGRESSION ALGORITHMS\n",
    "#models.append(('LR', LogisticRegression()))\n",
    "#models.append(('DTR', DecisionTreeRegressor()))\n",
    "#models.append(('SGDRegressor', linear_model.SGDRegressor())) \n",
    "#models.append(('BayesianRidge', linear_model.BayesianRidge()))\n",
    "#models.append(('LassoLars', linear_model.LassoLars())) \n",
    "#models.append(('ARDRegression', linear_model.ARDRegression())) \n",
    "#models.append(('PassiveAggressiveRegressor', linear_model.PassiveAggressiveRegressor())) \n",
    "#models.append(('TheilSenRegressor', linear_model.TheilSenRegressor()))\n",
    "#models.append(('LinearRegression', linear_model.LinearRegression())) \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "num_folds = 3\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Prepare the configuration to run the test\n",
    "seed = 7\n",
    "results = []\n",
    "names = []\n",
    "X = train_set_scaled\n",
    "y = train_set_labels\n",
    "\n",
    "# Every algorithm is tested and results are\n",
    "# collected and printed\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(\n",
    "        n_splits=3, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(\n",
    "        model, X, y, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (\n",
    "        name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n",
    "#ONCE CHOSEN THE ALGORITHM, WE DO SCALING, PARAM GRID K FOLD AND GRID SEARCH #################\n",
    "# Build a scaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "# Build parameter grid\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)\n",
    "\n",
    "# Build the model\n",
    "model = SVC()\n",
    "kfold = KFold(n_splits = num_folds, random_state = seed)\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, scoring = scoring, cv = kfold)\n",
    "grid_result = grid.fit(rescaledX, y_train)\n",
    "\n",
    "# Show the results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "#MODEL IS NOW READY TO BE SAVED AND USED\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Finding best parameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://www.andreagrandi.it/2018/04/14/machine-learning-pima-indians-diabetes/\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1.0, 10.0, 50.0],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'shrinking': [True, False],\n",
    "    'gamma': ['auto', 1, 0.1],\n",
    "    'coef0': [0.0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "model_svc = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(model_svc, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(train_set_scaled, train_set_labels)\n",
    "\n",
    "# Print the bext score found\n",
    "grid_search.best_score_\n",
    "\n",
    "\n",
    "# Create an instance of the algorithm using parameters\n",
    "# from best_estimator_ property\n",
    "svc = grid_search.best_estimator_\n",
    "\n",
    "# Use the whole dataset to train the model\n",
    "X = np.append(train_set_scaled, test_set_scaled, axis=0)\n",
    "Y = np.append(train_set_labels, test_set_labels, axis=0)\n",
    "\n",
    "# Train the model\n",
    "svc.fit(X, Y)\n",
    "\n",
    "\n",
    "#MAKING A PREDICTION\n",
    "# We create a new (fake) person having the three most correated values high\n",
    "new_df = pd.DataFrame([[6, 168, 72, 35, 0, 43.6, 0.627, 65]])\n",
    "# We scale those values like the others\n",
    "new_df_scaled = scaler.transform(new_df)\n",
    "\n",
    "# We predict the outcome\n",
    "prediction = svc.predict(new_df_scaled)\n",
    "\n",
    "\n",
    "# A value of \"1\" means that this person is likley to have type 2 diabetes\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using prediction pipeline with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10, iid=False)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print((\"best logistic regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regularization loss: how much we penalize the model \\nfor having large parameters that heavily weight certain features'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GRADIENT DESCENT = iterative technique to minimize complex loss function\n",
    "\n",
    "'''Two ways to combat overfitting:\n",
    "1. Use more training data. The more you have, the harder it is to overfit\n",
    "the data by learning too much from any single training example.\n",
    "\n",
    "2. Use regularization. Add in a penalty in the loss function for building\n",
    "a model that assigns too much explanatory power to any one feature or\n",
    "allows too many features to be taken into account.'''\n",
    "\n",
    "'''Regularization loss: how much we penalize the model \n",
    "for having large parameters that heavily weight certain features'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIST of sklearn CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#List of sklearn classifiers\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm.classes import OneClassSVM\n",
    "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "from sklearn.neighbors.classification import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.linear_model.ridge import RidgeClassifierCV\n",
    "from sklearn.linear_model.ridge import RidgeClassifier\n",
    "from sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier    \n",
    "from sklearn.gaussian_process.gpc import GaussianProcessClassifier\n",
    "from sklearn.ensemble.voting_classifier import VotingClassifier\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import DPGMM\n",
    "from sklearn.mixture import GMM \n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import VBGMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLTK text classification / pos-neg sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 99,   0],\n",
       "       [100,   1]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Natural language processing. classification: pos or neg\n",
    "#Sourcce: https://www.youtube.com/watch?v=9VK4lWuD45U&list=PL2iM-fIRjbTBFazzQ5uEzeASpmP8o40y1&index=51\n",
    "#https://github.com/akky2892/Machine-Learning-The-future/blob/master/Natural%20Language%20Processing/Natural%20Language%20Processing.py\n",
    "'''\n",
    "Natural language processing\n",
    "'''\n",
    "\n",
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing the dataset\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/akky2892/Machine-Learning-The-future/master/Natural%20Language%20Processing/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "#Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^A-Za-z]', '', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "\n",
    "#Creating a bag-of-words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "#Spliting into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#Fitting Naive Bayes to training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Predicting test set result\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#Making confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 3.6181 - acc: 0.6380\n",
      "Epoch 2/20\n",
      "768/768 [==============================] - 0s 90us/step - loss: 1.8647 - acc: 0.5599\n",
      "Epoch 3/20\n",
      "768/768 [==============================] - 0s 102us/step - loss: 1.3050 - acc: 0.5534\n",
      "Epoch 4/20\n",
      "768/768 [==============================] - 0s 110us/step - loss: 1.0270 - acc: 0.5456\n",
      "Epoch 5/20\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.9345 - acc: 0.5326\n",
      "Epoch 6/20\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.8688 - acc: 0.5260\n",
      "Epoch 7/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.8265 - acc: 0.5130\n",
      "Epoch 8/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.7864 - acc: 0.5313\n",
      "Epoch 9/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.7502 - acc: 0.6185\n",
      "Epoch 10/20\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.6880 - acc: 0.6615\n",
      "Epoch 11/20\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.6715 - acc: 0.6667\n",
      "Epoch 12/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6542 - acc: 0.6719\n",
      "Epoch 13/20\n",
      "768/768 [==============================] - 0s 142us/step - loss: 0.6459 - acc: 0.6523\n",
      "Epoch 14/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6415 - acc: 0.6549\n",
      "Epoch 15/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6389 - acc: 0.6654\n",
      "Epoch 16/20\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.6329 - acc: 0.6563\n",
      "Epoch 17/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6254 - acc: 0.6510\n",
      "Epoch 18/20\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6216 - acc: 0.6563\n",
      "Epoch 19/20\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.6182 - acc: 0.6589\n",
      "Epoch 20/20\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.6172 - acc: 0.6680\n",
      "768/768 [==============================] - 0s 514us/step\n",
      "Accuracy: 66.41\n"
     ]
    }
   ],
   "source": [
    "#Basic Keras neural network\n",
    "#Source: https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "# load the dataset\n",
    "dataset = loadtxt(r'D:\\Downloads\\DATASETS\\general\\pima_indian_dataset.csv', delimiter=',')\n",
    "\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=20, batch_size=40)\n",
    "\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
